resource_registry:
  OS::TripleO::NodeUserData: /home/stack/wipe-disks.yaml

parameter_defaults:
  LocalCephAnsibleFetchDirectoryBackup: /tmp/fetch_dir
  CephPoolDefaultSize: 1
  # when deploying a small number of osd's - < 12), it's necessary to decrease the default pg_num from 128 to get past the max 200pgs/per osd  limitation
  CephPoolDefaultPgNum: 32
  CephAnsiblePlaybookVerbosity: 1
  CephAnsibleDisksConfig:
    devices:
      - /dev/disk/by-path/pci-0000:00:17.0-ata-1
      - /dev/disk/by-path/pci-0000:00:17.0-ata-2
      - /dev/disk/by-path/pci-0000:00:17.0-ata-3
      - /dev/disk/by-path/pci-0000:00:17.0-ata-4
      - /dev/disk/by-path/pci-0000:01:00.0-nvme-1
      - /dev/disk/by-path/pci-0000:19:00.0-nvme-1
      - /dev/disk/by-path/pci-0000:86:00.0-nvme-1

# the following two parameters are the defaults. Just included them here for info
    osd_scenario: lvm
    osd_objectstore: bluestore
  CephAnsibleExtraConfig:
    osd_pool_default_autoscale_mode: on

  ExtraConfig:
    ceph::profile::params::osd_pool_default_pg_num: 32
    ceph::profile::params::osd_pool_default_pgp_num: 32
